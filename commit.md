birpc 意为: bi-direction rpc, 这是一个基于zk实现的rpc框架

目前以zk作为注册发现服务. 因为目前的业务侧用于CAP理论中的AP, 也就是可用性以及分区容错性, 但是zk注重CP
一致性以及分区容错性, 所以目前来说zk作为注册中心不太适合我们的业务. 这个替换工作留在以后去处理.

基本思路:
1. 基于protobuf以及netty实现的rpc框架.这里之所以不使用protostuff是因为我们有一套工具协调.
2. client-server模式, 在消息处理上没有严格意义上的生产者消费者, 客户端和服务器. 只有请求发送者以及请求处理者.
3. 屏蔽了接口定义, 以协议号绑定处理函数的方式, 进行方法请求处理. 也就是说无需像dubbo那样提前定义接口. 本地调用.也不是restful那种风格的接口预定义. 接口跟协议号绑定, 通过协议号获取到绑定方法进行处理.

启动流程:
1. 连接zk服务器, 获取当前命名空间下的节点列表数据.
2. 当前信息按照类型注册到zk服务器
3. 根据zkClient监听拿到的节点信息, 

20210818 夜有所思,预计新增
1. 因为zk不支持LB, 所以如果想负载均衡, 则需要在客户端实现, 思路如下:  
  1.1 通过连接数自动分配, 每个服务节点维护所有连上来的客户端节点, 可以获取到连接数, 客户端请求连接时, 选择连接数最小的服务节点.  
  1.2 通过请求量自动分配, 每个服务节点记录消费的请求量, 客户端根据此数据选择压力最小的服务节点.  
  1.3 客户端负载, 比如客户端初始连接2, 记录请求数, 如果请求数过大或者请求增长过大, 客户端选择新的服务节点进行连接, 动态扩容新节点.
  当客户端负载降低, 则减少请求连接.  
2. 监控相关, 对于每个节点的请求量, 处理量, 成功数, 失败数, 错误数应当有记录, 并且可以直观的输出. 在某个节点有网络问题, 处理异常等情况时.可以邮件或者短信方式告知.
3. 分组策略, 分组在业务层进行分组, 每个节点对应一个类型, 如game节点类型1, battle为2, 那么所有game节点监听battle节点类型2, 拿到类型2对应的所有battle节点信息,
通过负载均衡策略连接battle节点. 达到预分组, 动态扩容, 动态回收的目的. 如果服务节点是有状态的, 考虑加上redis作为服务同步数据库, 节点之间的状态尽量避免共享.
4. 节点类型定义分配, 每个节点服务器, 监听同类型的节点组, 当注册新节点类型时, 拿到最新的节点数据, 增加自己节点数据, 修改同步至zkServer.
监听不同类型的节点服务器, 根据响应的类型做对应的逻辑.比如:
login, game, battle 三个类型节点
  4.1 login2启动, 从zkServer拿到最新的login类型的子节点列表[login1], 随后节点列表加入login2, [login1, login2], 更新至zkServer. 此为同类型的节点. 仅仅只是做数据修改.
  4.2 game1启动, 从zkServer获取到最新login节点[login1, login2], 假如默认连接数是1, 则找出合适的1个login节点进行连接.
  4.3 game2运行状态, login1节点启动, 如果login2此时没有达到默认连接数1, 则获取到login1节点, 建立tcp连接.
  4.4 节点之间不会互相影响, 以此例子来说, login1节点如果因为其他原因宕机, 对于game1来说, 只会断开连接, 服务器不会停止运行. 
  
20210910
1. 负载均衡-已实现
2. 监控相关-未开始
3. 分组策略-已实现
4. 节点类型定义分配-已实现
5. 身份验证-框架提供一套最基本的,基于cat-net的解码器,实现的身份验证.若业务层定义了新的解码器, 则业务层重写身份验证.
6. 


20211122
游戏内有些比较特殊的需求, 如: 某游戏节点组成跨服组, 这些跨服组活动共享数据, 目前以下几个思路较好的去实现
1. 对birpc进行扩展, 对其增加连接负载策略, 在业务层实现新的策略-新的节点尝试连接时, 判断是否在统一分组内, 如果是则允许连接, 不是则放弃连接.
	优点:
	1.1 只有在同一分组内的节点, 才可连接成功, 不会造成资源浪费, 节省资源
	1.2 有状态节点, 当前组数据, 在此节点缓存和持久化, 可以分摊持久化压力
	缺点:
	1.1 birpc的扩展有些麻烦, 如果跨服节点上的活动关闭, 如何通知已连接的节点关闭连接? 
	
2. 不扩展birpc, 所有跨服节点无状态, 数据从redis中进行获取, 每个跨服节点都保持相同的逻辑
	优点: 
	2.1 无需扩展当前rpc框架, 直接动态支持
	缺点:
	2.1 无状态节点, 最终数据存储到redis, 存储压力大
	
	







